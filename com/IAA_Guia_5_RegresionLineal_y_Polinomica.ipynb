{"cells":[{"cell_type":"markdown","id":"c589f8bd-bc45-40bf-ba25-266d249c4b3a","metadata":{"id":"c589f8bd-bc45-40bf-ba25-266d249c4b3a","tags":[]},"source":["# Regresión Lineal\n","\n","En este notebook comenzamos a trabajar en los problemas de **Regresión**. Un problema de regresión consiste en aprender a predecir una *etiqueta* *Y* continua o cuantitativa a partir de un conjunto de atributos  **X**, tomando como muestra un conjunto de instancias. El modelo más común de regresión es la regresión lineal. Los modelos lineales se encuentran entre los modelos más simples, pero siguen siendo extremadamente comunes y útiles. Tienen algunas propiedades analíticas simples y son extremadamente fáciles de entrenar e interpretar. Además, son la base para un montón de modelos más complejos y modernos. Su importancia no debe ser subestimada.\n","\n","## 1. Regresión Lineal - 1D\n","\n","En la materia Introducción a la Ciencia de Datos ya han tenido un primer acercamiento a la regresión lineal. Aquí vamos a hacer un repaso de los principales conceptos que utilizaremos. Supongamos que queremos predecir una variable cuantitativa $Y$ como función de una única variable (por ahora, asumimos también cuantitativa) $X$. El modelo lineal asume que entre esas variables existe una relación del tipo\n","\n","$$Y \\approx \\omega_0 + \\omega_1 X,$$\n","\n","donde $\\beta_0$ y $\\beta_1$ son los parámetros del modelo, en este caso conocidos como *ordenada al origen* y *pendiente*, respectivamente. Estos parámetros son, hasta que no utilicemos los datos, parámetros desconocidos del modelo, que debemos *ajustar*. Otra forma que a veces pueden encontrar para el modelo lineal es la siguiente:\n","\n","$$Y = \\omega_0 + \\omega_1 X + \\epsilon, $$\n","\n","donde $\\epsilon$ es un término de error del modelo. A primera vista, este término puede ser muy confuso, pero esperamos que pronto quede claro qué significa. Notar que, en cualquiera de las dos formas, la variable predictora $X$ aparece de forma *lineal*, es decir, es decir, no está elevada al cuadrado, dentro de una raíz cuadrada, dentro de un seno, etc. Veremos en clases posteriores que este modelo es mucho más flexible de lo que parece. Tal vez les sorprenda leer ahora que podemos utilizar el modelo lineal para obtener relaciones no-lineales.\n","\n","**Referencias útiles**:\n","\n","* Capítulo 3 de *An Introduction to Statistical Learning*. Lo pueden obtener [aquí](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf).\n","* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/limo.html)\n","\n","\n","**Repaso**\n","\n","1. ¿Cuáles son las ventajas de utilizar un modelo lineal?¿Y las desventajas?\n","1. ¿Cuáles son las hipótesis del modelo lineal? Si no se cumplen, ¿significa que no podemos usar una regresión?"]},{"cell_type":"markdown","id":"41bb95e3-5e7e-4530-b091-3358e66916d8","metadata":{"id":"41bb95e3-5e7e-4530-b091-3358e66916d8","tags":[]},"source":["#### Dataset Sintético\n","\n","Vamos a comenzar generando un dataset sintético. Esto quiere decir que vamos a conocer la verdadera relación entre nuestra variable objetivo $Y$ y nuestra variable predictora $X$. Esto NO es lo común, pero lo hacemos para que puedan ver el efecto de algunas características del proceso."]},{"cell_type":"code","execution_count":null,"id":"a71902a7-35aa-4f82-9b3c-46b77c49abd9","metadata":{"id":"a71902a7-35aa-4f82-9b3c-46b77c49abd9"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"2dfa075d-df87-41ca-a046-7bf7116e3e25","metadata":{"id":"2dfa075d-df87-41ca-a046-7bf7116e3e25"},"source":["La siguiente celda genera nuestro dataset sintético. Por ahora, serán 1000 puntos provenientes de una relación lineal 1-D de la forma $y = 3x - 2$. Asegúrate de identificar correctamente la pendiente ($\\beta_1$) y la ordenada al origen ($\\beta_0$)."]},{"cell_type":"code","execution_count":null,"id":"1ba77c32-443d-49c9-8622-c54aa564918b","metadata":{"id":"1ba77c32-443d-49c9-8622-c54aa564918b"},"outputs":[],"source":["n = 1000\n","X = np.linspace(-2,3,n)\n","y_real = 3*X - 2"]},{"cell_type":"markdown","id":"b6f0b23a-955c-4bf1-bbcb-08538c4c374b","metadata":{"id":"b6f0b23a-955c-4bf1-bbcb-08538c4c374b"},"source":["Sin embargo, esta relación no es muy realista, ya que en cualquier proceso de medición suele introducirse *ruido*. ¿Dónde se origina ese ruido? Bueno, podríamos escribir mucho al respecto, desde detalles técnicos hasta aspectos filosóficos. Por ahora vamos a decir que depende de cada proceso, a veces lo podemos reducir, a veces no. Vamos a experimentar introduciendo ruido gaussiano, de forma de acercanos a un proceso de medición realista."]},{"cell_type":"code","execution_count":null,"id":"5a4ed326-c8ea-423c-9414-0123a87b3ab1","metadata":{"id":"5a4ed326-c8ea-423c-9414-0123a87b3ab1"},"outputs":[],"source":["np.random.seed(2022)\n","alpha = 1\n","y = y_real + alpha*np.random.randn(n)"]},{"cell_type":"markdown","id":"0260eab9-38ce-46e3-a3e7-f435b04c748f","metadata":{"id":"0260eab9-38ce-46e3-a3e7-f435b04c748f"},"source":["Graficamos los puntos obtenidos junto con la curva teórica."]},{"cell_type":"code","execution_count":null,"id":"440f7839-ed52-4d78-985e-8fcd035da9db","metadata":{"id":"440f7839-ed52-4d78-985e-8fcd035da9db"},"outputs":[],"source":["plt.scatter(X,y, s=2, alpha=0.5, label='Datos Medidos')\n","plt.plot(X, y_real, '--',label='Curva Teórica', c='r')\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"1598747a-6532-450c-80cc-7cbf91dbcff4","metadata":{"id":"1598747a-6532-450c-80cc-7cbf91dbcff4","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["Notar que, si bien los puntos no están sobre la línea teórica - debido al ruido -, la relación lineal parece mantenerse muy bien.\n","\n","**Ejercicio:** jugar con el código introduciento diferentes niveles de ruido modificando la variable `alpha`. ¿Cuándo crees, a simple vista, que la relación lineal ya queda oculta por el ruido?"]},{"cell_type":"markdown","id":"0b975cd5-c8e0-4e63-a742-b8f1a7e23e08","metadata":{"id":"0b975cd5-c8e0-4e63-a742-b8f1a7e23e08"},"source":["Empecemos notando algo. Cualquier método para ajustar datos arranca con ciertos postulados sobre las características de esos datos. Para la regresión lineal son la linealidad, normalidad, homocedasticidad, independencia, ausencia de multicolinealidad, etc. Cuando estas condiciones se cumplen, el método es óptimo, y los resultados que se desprenden del método tienen total validez. Entonces, antes de ajustar una regresión lineal, deberíamos chequear si esas hipótesis se cumplen. Sin embargo, salvo en contadas ocasiones, es muy difícil tener absoluta certeza de que así sea. En general, no hay ningún motivo para suponer que esas condiciones se cumplan exactamente y, en muchos casos, ni siquiera aproximadamente. ¿Entonces significa que no podemos usar el método? Por suerte, no. Si no se cumplen, eso no significa que el método sea malo. Probablemente haya uno mejor - que en general no sabemos cuál es - y debemos tener particular cuidado con las conclusiones estadísticas que desprendamos del método. Además, exiten  métodos que se proponen con muy pocas hipótesis y que no tienen garantía de optimalidad teórica. Otros métodos son óptimos en condiciones muy generales, pero solo cuando el tamaño de muestra tiende a infinito (es decir, cuando es muy grande nuestro conjunto de datos). En el mundo real, no hay ningún método que sea mejor que todos los demás en todos los casos.\n","\n","Pero hay algo más que también es importante diferenciar. Algunas veces, cuando estamos estudiando cierto fenómeno, nuestro objetivo principal no es modelar su naturaleza y entenderlo, sino simplemente tener poder predictivo sobre lo que va a ocurrir. Entonces, ¿qué podemos hacer en este mundo de hipótesis, métodos y objetivos aparentemente disímeles? Un enfoque complementario - típico de la comunidad de Aprendizaje Automática y expresado de manera simplificada - es probar un método y \"si funciona, funciona\". Pero, ¿cuándo un método funciona? Cuando tiene **poder predictivo**. Nuestro objetivo principal ya no va a ser modelar y comprender la realidad, sino simplemente tener buenas predicciones."]},{"cell_type":"markdown","id":"3de11ea0-cecf-41ff-8493-2d06ee2beea9","metadata":{"id":"3de11ea0-cecf-41ff-8493-2d06ee2beea9"},"source":["### 1.2 Regresión lineal con Scikit-Learn\n","\n","\n","Vamos a ajustar estos datos con la regresión lineal de Scikit-Learn. **Es recomendable consultar la [documentación sobre regresión lineal de esa librería](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).**\n","\n","1. Definición de un objeto modelo `linear_model` (o como ustedes prefieran llamarlo, es indistinto)."]},{"cell_type":"code","execution_count":null,"id":"72943aa5-cf84-4b98-adfd-2b4b4a982d4a","metadata":{"id":"72943aa5-cf84-4b98-adfd-2b4b4a982d4a"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"6b1e2d84-faf2-47a4-b2df-ebe368846567","metadata":{"id":"6b1e2d84-faf2-47a4-b2df-ebe368846567"},"source":["2. Ajuste a los datos (`.fit(X,y)`).\n","\n","**Nota:** cuando trabajen con un solo atributo (X tiene un solo atributo), se van a topar con un error típico. Scikit-Learn espera que X tenga la forma de *número de instancias* x *número de atributos*, entonces, debemos llevar X a esa forma bidimencional utilizando el método `reshape`."]},{"cell_type":"code","execution_count":null,"id":"e96dcfe6-fdd1-471f-a435-d55b89e004b4","metadata":{"id":"e96dcfe6-fdd1-471f-a435-d55b89e004b4"},"outputs":[],"source":["print(X.shape)\n","X = X.reshape(-1,1)\n","print(X.shape)"]},{"cell_type":"markdown","id":"5605f74d-fa27-4b67-99d2-90528373789d","metadata":{"id":"5605f74d-fa27-4b67-99d2-90528373789d"},"source":["Ya podemos ajustar"]},{"cell_type":"code","execution_count":null,"id":"467b7614-026b-4cef-b42a-4cc9ed9267e1","metadata":{"id":"467b7614-026b-4cef-b42a-4cc9ed9267e1"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"2f7b0486-a245-4bbf-acb2-f9572779cadf","metadata":{"id":"2f7b0486-a245-4bbf-acb2-f9572779cadf"},"source":["Obtenga los pesos del modelo lineal.\n","\n","*Pista:*\n","- *Considere los atributos `linear_model.coef_` y `linear_model.intercept_` del regresor lineal."]},{"cell_type":"code","execution_count":null,"id":"ef336618-9d14-42ab-ab61-a551832ee8c8","metadata":{"id":"ef336618-9d14-42ab-ab61-a551832ee8c8","outputId":"037fb5ff-957d-4770-ac6d-1b0821aa5cbb"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"d03431ff-716b-4df6-8861-ce8d997a1c15","metadata":{"id":"d03431ff-716b-4df6-8861-ce8d997a1c15"},"source":["Notar que la pendiente es parte de un `array`... Eso es una pista de algo que se viene después. Ahora, ¿Cómo podemos hacer para saber si el ajuste es bueno?\n","\n","En primer lugar, veamos otra cosa que podemos hacer con el objeto `linear_model`. Podemos hacer predicciones.\n","\n","3. Predicciones (`.predict(X)`)."]},{"cell_type":"code","execution_count":null,"id":"0a00e801-4c95-4016-8069-7627cf29d205","metadata":{"id":"0a00e801-4c95-4016-8069-7627cf29d205"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"b2f0eb88-e752-4b3c-b963-b330ae1e5d35","metadata":{"id":"b2f0eb88-e752-4b3c-b963-b330ae1e5d35"},"source":["Nuevamente, veamos el resultado gráficamente."]},{"cell_type":"code","execution_count":null,"id":"a123560c-c21f-4a29-8e5d-b261a125616e","metadata":{"id":"a123560c-c21f-4a29-8e5d-b261a125616e"},"outputs":[],"source":["plt.scatter(X,y, s=2, alpha=0.5, label='Datos Medidos')\n","plt.plot(X, y_real, '--',label='Curva Teórica', c='r')\n","plt.plot(X, y_pred, '--', lw=2, label='Curva Obtenida (Scikit-Learn)', c='g')\n","\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"63a96e4a-b30a-4388-b65d-ce5712161efc","metadata":{"id":"63a96e4a-b30a-4388-b65d-ce5712161efc"},"source":["### 1.3 Evaluación\n","\n","La evaluación de un modelo es algo tan importante como su entrenamiento. Lo primero que podemos hacer es comparar las predicciones obtenidas con los valores a predecir. Si el modelo está haciendo un buen trabajo, éstas deberían estar agrupadas alrededor de la identidad (la recta diagonal $y=x$)."]},{"cell_type":"markdown","id":"7e3ffd46-aa4e-4ddf-94b7-5f8f83ddca03","metadata":{"id":"7e3ffd46-aa4e-4ddf-94b7-5f8f83ddca03"},"source":["#### Métricas\n","\n","Otra forma de evaluar un modelo - sumamente importante y útil - es calcular una métrica de desempeño. En los problemas de regresión es común utilizar métricas como el **error cuadrático medio**:\n","\n","$$ MSE(y, \\hat y) = \\frac{1}{M}\\sum_{i=1}^M (y_i - \\hat y_i)^2 $$\n","\n","donde $M$ es la cantidad de instancias e $\\hat y$ son las predicciones del modelo. Notar que estamos comparando cada dato y predicción de manera individual.\n","\n","**Para Pensar:** si tuvieras que decir de qué parámetros del modelo depende el error cuadrático medio (MSE), ¿qué dirías?\n","\n","Otra métrica muy utilizada, y un poco más amigable, es la **raiz del error cuadrático medio**, ya que tiene las mismas unidades que la variable obejtivo $y$\n","\n","$$ RMSE(y, \\hat y) = \\sqrt{MSE(y, \\hat y)} $$\n","\n","Notar que la evaluación de un modelo siempre consiste en comparar los valores predichos $\\hat y$ y los valores objetivo $y$, utilizando alguna métrica."]},{"cell_type":"code","execution_count":null,"id":"c3fd0325-7dc2-4cef-9c79-415813f253a7","metadata":{"id":"c3fd0325-7dc2-4cef-9c79-415813f253a7"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","def root_mean_squared_error(y, y_pred):\n","    return np.sqrt(mean_squared_error(y, y_pred))\n","\n","print(f'MSE = {mean_squared_error(y,y_pred)}')\n","print(f'RMSE = {root_mean_squared_error(y,y_pred)}')"]},{"cell_type":"markdown","id":"d106c600-9b2f-4ebf-a890-9a4dfdd3f669","metadata":{"id":"d106c600-9b2f-4ebf-a890-9a4dfdd3f669"},"source":["En este caso, por una cuestión de magnitudes, dan muy parecido. Modifica `alpha` (por ejemplo, 2, 5 y 10) y vuelve a correr. ¿Qué ocurre con el RMSE? También observa el gráfico y trata de ver si puedes interpretar el valor del RMSE gráficamente.\n","\n","Existen muchas métricas para evaluar un modelo de regresión. Cuál será conveniente dependerá del objetivo de nuestro análisis. Pero notar que, en este caso, MSE es la misma métrica que utiliza Scikit-Learn para obtener los coeficientes del ajuste lineal. Es decir, estamos evaluando con la misma métrica que usamos para entrenar. Muchas veces esto **no será así.**\n","\n","#### Residuos\n","\n","Un último análisis que podemos hacer es el análisis de residuos. Esto consiste en ver la diferencia entre nuestra predicción y el valor real. El residuo se define como\n","\n","$$ \\text{res} = y - \\hat y $$\n","\n","Dos gráficos usuales que podemos hacer con los residuos son:\n","\n","1. Su histograma"]},{"cell_type":"code","execution_count":null,"id":"f7728b72-f2d2-4dc1-b309-3b7e281e41c5","metadata":{"id":"f7728b72-f2d2-4dc1-b309-3b7e281e41c5"},"outputs":[],"source":["res = y - y_pred\n","plt.hist(res, bins = 20, rwidth = 0.9)\n","plt.xlabel('res')\n","plt.show()"]},{"cell_type":"markdown","id":"0a2290a5-fe62-4e6f-87de-300124ef156e","metadata":{"id":"0a2290a5-fe62-4e6f-87de-300124ef156e"},"source":["En el enfoque clásico de regresión lineal, los residuos deben tener media cero y estar distribuidos normalmente. En la práctica, esto no suele suceder, ya que son necesarias muchas hipótesis para que así sea. Sin embargo, observar este gráfico puede darnos indicios de la calidad de nuestro modelo.\n","\n","**Para pensar:** ¿Cuál te parece que es el *ancho* de esta distribución y con qué valor visto puede estar relacionado?\n","\n","2. También podemos graficar la relación entre el residuos y el valor correspondiente de $X$"]},{"cell_type":"code","execution_count":null,"id":"e47d1cce-99a9-43ed-96ce-dde8e9d188c0","metadata":{"id":"e47d1cce-99a9-43ed-96ce-dde8e9d188c0"},"outputs":[],"source":["plt.plot(X, res)\n","plt.xlabel('X')\n","plt.ylabel('res')\n","plt.show()"]},{"cell_type":"markdown","id":"d696b3dc-5b28-43a8-ac1c-8a2b59419444","metadata":{"id":"d696b3dc-5b28-43a8-ac1c-8a2b59419444"},"source":["Este gráfico también suele ser muy útil, ya que nos da una idea sobre en qué regiones de X el modelo anda mejor y en cuáles peor. A medida que hay más atributos predictores, este gráfico es más difícil de interpretar.\n","\n","\n","## 2. Regresión Lineal - 2D\n","\n","La generalización de la regresión lineal a más atributos es muy sencilla. Por ejemplo, para dos atributos, $X_1$ y $X_2$, la forma es\n","\n","$$Y \\approx \\omega_0 + \\omega_1 X_1 + \\omega_2 X_2.$$\n","\n","Y, en el caso de $p$ variables predictoras,\n","\n","$$Y \\approx \\omega_0 + \\omega_1 X_1 + \\omega_2 X_2 + \\omega_3 X_3 + ... + \\omega_p X_p.$$\n","\n","Es decir, debemos encontrar una pendiente por cada atributo, pero sigue siendo una única ordenada al origen.\n","\n","Ahora, replicaremos el mismo análisis, pero para una relación lineal con dos atributos, $y = -3x_1 + 2x_2 + 4$. Nuevamente, sumaremos algo de ruido para hacerlo más realista."]},{"cell_type":"code","execution_count":null,"id":"89984720-5ef3-438a-a07f-1d404aca96d1","metadata":{"id":"89984720-5ef3-438a-a07f-1d404aca96d1"},"outputs":[],"source":["n = 1000\n","x1 = np.random.rand(n)\n","x2 = np.random.rand(n)\n","y = -3*x1 + 2*x2 + 4 + 0.25*np.random.randn(n)"]},{"cell_type":"markdown","id":"3cc2d396-3563-4463-a776-0e932d2ff5c5","metadata":{"id":"3cc2d396-3563-4463-a776-0e932d2ff5c5"},"source":["Visualizamos. El código es un poco más complejo, no te preocupes si no lo entiendes."]},{"cell_type":"code","execution_count":null,"id":"0a2ad19b-fc44-4a01-8325-875704e6de35","metadata":{"id":"0a2ad19b-fc44-4a01-8325-875704e6de35"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","def plot_3d_scatter(x1, x2, y, ax=None, fig = None):\n","    if (fig is None) and (ax is None):\n","        fig = plt.figure(figsize = (8,6))\n","        ax = fig.add_subplot(111, projection='3d')\n","    ax.scatter(x1, x2, y)\n","\n","    ax.set_xlabel('x1')\n","    ax.set_ylabel('x2')\n","    ax.set_zlabel('y')\n","\n","plot_3d_scatter(x1, x2, y, ax=None, fig = None)"]},{"cell_type":"markdown","id":"6be75999-f443-4ffc-96c8-8b0bfc1bc491","metadata":{"id":"6be75999-f443-4ffc-96c8-8b0bfc1bc491"},"source":["Creamos los atributos y hacemos un `train_test_split`"]},{"cell_type":"code","execution_count":null,"id":"0b99c6e9-764f-4c6e-aaf6-3486d1da41b4","metadata":{"id":"0b99c6e9-764f-4c6e-aaf6-3486d1da41b4","outputId":"82cd4434-b87b-4983-cb6f-aae80624a11d"},"outputs":[],"source":["X = np.vstack((x1,x2)).T #Uno a x1 y x2\n","print(X.shape)"]},{"cell_type":"code","execution_count":null,"id":"d225505b","metadata":{"id":"d225505b"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"599ac45b-d495-46b9-a5fb-add719b29255","metadata":{"id":"599ac45b-d495-46b9-a5fb-add719b29255"},"source":["Notar que tiene el `shape` que necesita Scikit-Learn. Definimos los modelos y entrenamos"]},{"cell_type":"code","execution_count":null,"id":"e0116ec2-8dda-4817-ad44-bbadf3259523","metadata":{"id":"e0116ec2-8dda-4817-ad44-bbadf3259523"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"a0e5bcb3-a50b-4c0f-b64f-bb1e6d35c3ea","metadata":{"id":"a0e5bcb3-a50b-4c0f-b64f-bb1e6d35c3ea"},"source":["Nuevamente, veamos qué aprendió. Obtenga **las pendientes** y la ordenada al origen"]},{"cell_type":"code","execution_count":null,"id":"f7339ae1-d029-4ab2-a3cb-ddac9b18cf40","metadata":{"id":"f7339ae1-d029-4ab2-a3cb-ddac9b18cf40"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"59f2e9eb-739b-4dc2-97bd-b62bd121f388","metadata":{"id":"59f2e9eb-739b-4dc2-97bd-b62bd121f388"},"source":["En este caso, la curva que aproxima a los datos no es más una curva, sino una superficie. La siguiente función nos ayuda a graficar. Nuevamente, no se preocupen si no entienden del todo lo que hace."]},{"cell_type":"code","execution_count":null,"id":"944ae603-e2b9-4103-a2de-ef086cd95c31","metadata":{"id":"944ae603-e2b9-4103-a2de-ef086cd95c31"},"outputs":[],"source":["from matplotlib import cm\n","\n","def plot_3d_regressor(x1_min, x1_max, x2_min,x2_max, N, regressor, ax=None, fig = None):\n","    x1 = np.linspace(x1_min,x1_max,N)\n","    x2 = np.linspace(x2_min,x2_max,N)\n","    X1, X2 = np.meshgrid(x1,x2)\n","\n","    y = regressor.predict(np.array([X1.ravel(), X2.ravel()]).T)\n","    Y = y.reshape(X1.shape)\n","\n","    if (fig is None) and (ax is None):\n","        fig = plt.figure()\n","        ax = fig.add_subplot(111, projection='3d')\n","\n","    surf = ax.plot_surface(X1, X2, Y, cmap=cm.coolwarm,\n","                       linewidth=0, antialiased=False, alpha = 0.5)"]},{"cell_type":"code","execution_count":null,"id":"22c4f9a3-04b5-4239-a513-c8cb170c6dc0","metadata":{"id":"22c4f9a3-04b5-4239-a513-c8cb170c6dc0"},"outputs":[],"source":["fig = plt.figure(figsize = (6,6))\n","ax = fig.add_subplot(111, projection='3d')\n","plt.title('Regresión Lineal')\n","plot_3d_regressor(0, 1, 0, 1, 100, linear_model, ax, fig)\n","plot_3d_scatter(x1, x2, y, ax, fig)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"8df0e0a3-dd51-4974-b157-96aa42d45825","metadata":{"id":"8df0e0a3-dd51-4974-b157-96aa42d45825"},"source":["Nuevamente, podemos hacer una evaluación de los resultados. Empecemos haciendo las predicciones."]},{"cell_type":"code","execution_count":null,"id":"13113039-5143-43b8-b7b1-ffc35901bc95","metadata":{"id":"13113039-5143-43b8-b7b1-ffc35901bc95"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"94454a09-9a12-4d63-b01b-dcfa3faad5af","metadata":{"id":"94454a09-9a12-4d63-b01b-dcfa3faad5af"},"source":["Y repetimos la evaluación vista anteriormente."]},{"cell_type":"code","execution_count":null,"id":"d0e27727-c040-4a62-ae61-0a0014e64e0c","metadata":{"id":"d0e27727-c040-4a62-ae61-0a0014e64e0c"},"outputs":[],"source":["plt.scatter(y_train,y_pred, alpha=0.75, s=1)\n","plt.plot([0,7],[0,7], ls='--', c='r', label='identidad')\n","plt.xlabel('y')\n","plt.ylabel('y_pred')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"54c221c0","metadata":{"id":"54c221c0"},"source":["Calcule MSE y RMSE"]},{"cell_type":"code","execution_count":null,"id":"698b30e1-996c-4eec-a906-e2fba4175903","metadata":{"id":"698b30e1-996c-4eec-a906-e2fba4175903"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"2c0c91d5","metadata":{"id":"2c0c91d5"},"source":["Finalmente, miremos los residuos:"]},{"cell_type":"code","execution_count":null,"id":"3207add2-b997-4577-ba50-556e32282444","metadata":{"id":"3207add2-b997-4577-ba50-556e32282444"},"outputs":[],"source":["res = y_train - y_pred\n","plt.hist(res, bins = 20, rwidth = 0.9)\n","plt.xlabel('res')\n","plt.show()"]},{"cell_type":"markdown","id":"c79d7854","metadata":{"id":"c79d7854"},"source":["# Regresión Polinomial"]},{"cell_type":"markdown","id":"76e9ca83","metadata":{"id":"76e9ca83"},"source":["Hasta ahora hemos utilizado modelos lineales para tareas de regresión y clasificación. Esto quiere decir que el resultado de nuestra predicción depende de una combinación lineal de los features. Sin embargo, hemos visto que es útil preprocesar los features antes de alimentar un modelo. Un preprocesado muy util, que permite capturar relaciones no lineales entre los features, es el _preprocesado polinómico_. El resultado de componer este preprocesado con un modelo lineal de regresión es lo que se conoce como _regresión polinómica_, pero también puede ser utilizado para clasificación con un regresor lineal.\n","\n","A lo largo de este notebook:\n","- Nos encontraremos con un conjunto de datos que posee características no lineales\n","- Entrenaremos un regresor lineal para clasificar\n","- Realizaremos un preprocesamiento polinómico de los datos, y veremos esto modifica nuestros datos.\n","- Entrenaremos un regresor lineal polinómico, y lo compararemos al regresor lineal.\n","- Exploraremos distintos grados del polinomio, en búsqueda de encontrar _el grado óptimo_.\n"]},{"cell_type":"markdown","id":"00c76168","metadata":{"id":"00c76168"},"source":["## Dataset"]},{"cell_type":"markdown","id":"2032cd43","metadata":{"id":"2032cd43"},"source":["La siguiente línea de código generará un dataset sintético a utilizar en este notebook:"]},{"cell_type":"code","execution_count":null,"id":"d94164ae","metadata":{"id":"d94164ae"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"8031038e","metadata":{"id":"8031038e"},"outputs":[],"source":["np.random.seed(42)\n","X = 6 * np.random.rand(500, 1) - 3\n","y = 0.5 * 2*X**2 + X + 2 + np.random.randn(500, 1)"]},{"cell_type":"markdown","id":"26d194dd","metadata":{"id":"26d194dd"},"source":["## Ejercicio 1\n","\n","- Examine el dataset y grafíquelo\n","- Utilice la función `train_test_split` para separar un 20% del dataset como conjunto de test. Este servirá para evaluar la performance del modelo sobre datos no vistos anteriormente."]},{"cell_type":"code","execution_count":null,"id":"252140ef","metadata":{"id":"252140ef"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"e26092ed","metadata":{"id":"e26092ed"},"source":["## Ejercicio 2\n","\n","- Entrene un regresor lineal a los datos de entrenamiento\n","- Genere las predicciones.\n","- Grafique el conjunto de datos con la línea de regresión.\n","- Calcule MSE y RMSE. Qué observa?"]},{"cell_type":"code","execution_count":null,"id":"d326f600","metadata":{"id":"d326f600"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"3306ddfb","metadata":{"id":"3306ddfb"},"source":["## Ejercicio 3\n","\n","Un polinomio de grado $M$ es una combinación lineal de monomios de grado $0$ hasta $M$. Es decir, si miramos un polinomio univariable de segundo grado, este es de la forma:\n","$$w_0 + w_1 x + w_2 x^2 $$\n","que no es mas que una combinacion lineal de los monomios $x^0$ ($=1$), $x^1$ ($=x$) y $x^2$.\n","\n","El caso de dos variables $x_1$ y $x_2$ (como nuestro dataset), tambien hay que tener en cuenta los terminos cruzados:\n","$$w_{(0,0)} + w_{(1,0)} x_1 + w_{(0,1)} x_2 + w_{(2,0)} x_1^2 + w_{(1,1)} x_1 x_2 + w_{(0,2)} x_2^2$$\n","\n","es decir una combinacion lineal de los monomios $1$, $x_1$, $x_2$, $x_1^2$, $x_2^2$ y $x_1 x_2$.\n","\n","Esto implica que un modelo polinomico no es mas que un modelo lineal, con un preprocesado que consiste en tomar los features del dataset, y devolver todos los monomios de estos hasta cierto grado.\n","\n","Este ya esta implementado en sklearn:\n","\n","\n","\n","- Importe de `sklearn.preprocessing` el transformer `PolynomialFeatures`.\n","- Instancielo con el grado polinomico 1, 2 y 3. Para cada uno de ellos, transforme los datos (_Pista: `.fit_transform()`_), y examine como cambia el dataset, en particular mire el `shape`.\n","\n","*Nota:*  Resulta crucial comprender que `PolynomialFeatures` es un _transformador_, no un _modelo de aprendizaje_ propiamente dicho. Aunque implemente métodos como `.fit()` y `.transform()`, su función principal es preparar o modificar los datos de entrada (features) para su posterior uso en modelos de aprendizaje. El método .fit() en el contexto de PolynomialFeatures no aprende o ajusta ningún parámetro del modelo a partir de los datos. En cambio, simplemente determina la estructura de los monomios (términos polinómicos) basándose en el grado especificado y las características de entrada. Por lo tanto, aunque utilizamos .fit() y .fit_transform(), su propósito es puramente de preprocesamiento: expandir el espacio de características para incluir términos polinómicos, lo que puede ayudar a modelos lineales simples a capturar relaciones no lineales dentro de los datos."]},{"cell_type":"code","execution_count":null,"id":"15f01cd9","metadata":{"id":"15f01cd9"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"da7f7396","metadata":{"id":"da7f7396"},"source":["## Ejercicio 4\n","\n","- Entrene un regresor lineal polinómico de grado 2 sobre el dataset de entrenamiento.\n","- Evalúe MSE y RMSE sobre los conjuntos de entrnamiento y evaluación. Compare con el obtenido en el ejercicio 2.\n","- Arme un pipeline de sklearn usando como primer paso el preprocesador polinomico, y como ultimo el regresor lineal. (_Pista: tanto `Pipeline` como `make_pipeline` de `sklearn.pipeline` pueden servir_)\n","- Entrene este pipeline sobre el conjunto de entrenamiento.\n","- Haga las predicciones.\n","- Grafique el conjunto de datos y la linea de regresión.\n","- Calcule MSE y RMSE. Qué observa?"]},{"cell_type":"code","execution_count":null,"id":"d9ae310a","metadata":{"id":"d9ae310a"},"outputs":[],"source":["#Completar"]},{"cell_type":"markdown","id":"e8c09217","metadata":{"id":"e8c09217"},"source":["## Ejercicio de Regresión Polinómica: Predicción de Precios de Viviendas\n","\n","Eres un analista de datos en una empresa inmobiliaria y se te ha proporcionado el conjunto de datos housing.csv que contiene información sobre diferentes propiedades y sus características. Tu tarea principal es predecir el precio de las viviendas basándote en sus características, pero en particular, estás interesado en investigar cómo el tamaño de la vivienda (por ejemplo, area si existe esa columna en el conjunto de datos) afecta el precio de manera no lineal.\n","\n","Instrucciones:\n","\n","1. Carga y Análisis Exploratorio de Datos: Comienza cargando el conjunto de datos Housing.csv. Realiza un análisis exploratorio básico y visualiza la relación entre el tamaño de las viviendas y sus precios.\n","\n","1. Regresión Lineal Simple: Como primer paso, modela la relación entre el tamaño de la vivienda y el precio utilizando regresión lineal simple. Evalúa el rendimiento de este modelo utilizando métricas adecuadas como el RMSE.\n","\n","1. Regresión Polinómica: Al observar los resultados y las visualizaciones del modelo lineal, considera aplicar una regresión polinómica. Transforma la característica del tamaño de la vivienda para incluir términos polinómicos y ajusta una regresión lineal a estos términos transformados.\n","\n","1. Comparación y Evaluación: Evalúa y compara el rendimiento del modelo polinómico con el modelo lineal simple. ¿Qué grado del polinomio proporciona el mejor ajuste para los datos sin sobreajustar?\n","\n","1. Visualización: Grafica las viviendas en función de su tamaño y precio. Añade las curvas de regresión para el modelo lineal y el modelo polinómico y compara visualmente cómo se ajustan a los datos.\n","\n","1. Reflexión Final: ¿En qué escenarios considerarías usar regresión polinómica sobre regresión lineal simple? ¿Hay alguna preocupación sobre el uso de polinomios de grado muy alto?\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset de EALS\n","\n","### Regresión Lineal Simple:\n","\n","1. Predicción de speaking_rate basada en alsfrsr_1 (habla): Estudiar cómo la capacidad de hablar (medida a través de alsfrsr_1) está relacionada con la velocidad a la que los usuarios pueden hablar.\n","\n","1. Predicción de intelligibility basada en bulbar (suma de las tres primeras preguntas relacionadas con el habla y el tracto vocal): Analizar la relación entre la claridad con la que los pacientes pueden hablar y su puntuación en la escala bulbar.\n","\n","### Regresión Lineal Múltiple:\n","\n","1. Predicción de alsfrsr (suma de las 12 preguntas) utilizando speaking_rate, intelligibility, y years_since_onset: Explorar cómo diferentes características relacionadas con el habla y el tiempo transcurrido desde el inicio de la enfermedad pueden predecir la puntuación general de la escala ALSFRS-R.\n","\n","1. Influencia del site_of_onset_bulbar en la intelligibility ajustando por alsfrsr: Investigar si el sitio de inicio de los síntomas (bulbar o no) tiene un impacto en la inteligibilidad del habla, controlando por la gravedad global de la enfermedad.\n","\n","### Regresión Polinomial:\n","\n","1. Predicción de speaking_rate utilizando polinomios de alsfrsr y alsfrsr_1: Estudiar si la relación entre la velocidad del habla y las puntuaciones en las escalas ALSFRS-R es no lineal.\n","\n","1. Modelado de intelligibility en función del tiempo (years_since_first_session): Determinar si la inteligibilidad del habla degrada de manera no lineal con el tiempo para los pacientes.\n","\n","### Análisis de Residuos y Diagnóstico de Modelos:\n","1. Para cualquiera de los modelos anteriores, realizar un análisis de residuos para evaluar la adecuación del modelo y la presencia de patrones no capturados por el modelo lineal o polinomial."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":5}
